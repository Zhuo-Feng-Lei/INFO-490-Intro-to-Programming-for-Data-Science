# -*- coding: utf-8 -*-
"""INFO490HP-P1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lTKwxTQ_lL1RnVslu2_3eGp4IbrYg4q2
"""

import numpy as np
import urllib
import re
import os
import spacy
import requests
import collections
import matplotlib.pyplot as plt
import nltk
from nltk.util import ngrams
from spacy.lang.en.stop_words import STOP_WORDS
# versions
PG_HAMLET_URL = 'http://www.gutenberg.org/cache/epub/2265/pg2265.txt'

def read_remote(url):
  # assumes the url is already encoded (see urllib.parse.urlencode)
  response = requests.get(url)
  if response.status_code == requests.codes.ok: # that is 200
    return response.text
  return None

def build_google_drive_url(doc_id):
  DRIVE1  = "https://docs.google.com/uc"
  DRIVE2  = "https://drive.google.com/uc"
  baseurl = DRIVE1 # DRIVE2 works as well 
  params = {"export" : "download",
            "id"     : doc_id}
  
  # build the url using baseurl
  # and the query parameters specified 
  # so that you can fetch it using
  # read_remote
  
  # CHANGE ME:
  url = baseurl + "?" + urllib.parse.urlencode(params) 
  return url

  
def get_harry_potter():
  g_id = get_book_id()
  url  = build_google_drive_url(g_id)
  return read_remote(url)
  
def get_book_id():
  return '1jOCDUhsMY3uAoLqV3NoZogyrr-FVqEoo'

def clean_hp(text):
  start = text.rfind('Harry Potter and the Sorcerer\'s Stone')
  return text[start:].strip()

def split_text_into_tokens(text):
  temp = re.findall(r"['A-Za-z0-9]+-?['A-Za-z0-9]+", text)
  normalized = []
  for i in temp:
    string = i.strip("!\"#$%&'()*+, -./:;<=>?@[\]^_`{|}~")
    if string.endswith("'s"):
      string = string[:len(string)-2]
    normalized.append(string)
  return normalized

def load_stop_words(add_pronouns = False):
  stopwords = list(STOP_WORDS)
  pronouns = ['they', 'your', "who", "she'd", "he'd", 'madam', 'he', "she", 'i', 'it', "i'm", "i've", "oh", "you", "mr", "mrs", "i'll", "i'd"]
  if add_pronouns == True:
    for i in pronouns:
      stopwords.append(i)
  return stopwords

def bi_grams(tokens):
  return nltk.ngrams(tokens, 2)

def top_n(tokens, n):
  counter = collections.Counter(tokens)
  return counter.most_common(n)

def find_characters_v1(text, stoplist = [], top = 15):
  cleaned = []
  tokens = split_text_into_tokens(text)
  r = re.compile(r"\b[A-Z].*?\b")
  caps = list(filter(r.match, tokens))
  for i in caps:
    if (i.lower() not in stoplist):
      cleaned.append(i)
  return top_n(cleaned,top)

def find_characters_v2(text, stoplist = [], top = 15):
  filtered = []
  new_bigrams = []
  tokens = split_text_into_tokens(text)
  bigrams = bi_grams(tokens)
  for i in bigrams:
    if (i[0][0].isupper() and i[1][0].isupper() and i[0].lower() not in stoplist and i[1].lower() not in stoplist):
      new_bigrams.append(f"{i[0]} {i[1]}")
  result = top_n(new_bigrams,top)
  return result

def find_characters_nlp(text, top = 15):
  nlp = spacy.load("en")
  doc = nlp(text)
  result = []
  for entity in doc.ents:
    if entity.label_ == "PERSON":
      name = entity.text
      result.append(name)
  return top_n(result,top)

def split_into_chapters(text):
  split = text.split('CHAPTER')[1:]
  return [f"CHAPTER{chapter}".strip() for chapter in split]

def get_character_counts_v1(chapters):

  harry = []
  ron = []
  hagrid = []
  hermione = []

  for i in range(len(chapters)):
    harry.append(np.char.count(chapters[i], "Harry"))
    ron.append(np.char.count(chapters[i], "Ron"))
    hagrid.append(np.char.count(chapters[i], "Hagrid"))
    hermione.append(np.char.count(chapters[i], "Hermione"))

  return np.array([harry, ron, hagrid, hermione])

def simple_graph_v1(plots):

  fig = plt.figure()
  subplot = fig.add_subplot(1,1,1)

  subplot.plot(plots[0])
  subplot.plot(plots[1])
  subplot.plot(plots[2])
  subplot.plot(plots[3]) 

  # this is important for testing
  return fig

def pipeline_v1():

  hp = clean_hp(get_harry_potter())
  chapters = split_into_chapters(hp)

  plots = get_character_counts_v1(chapters)
  fig = simple_graph_v1(plots)
  return fig