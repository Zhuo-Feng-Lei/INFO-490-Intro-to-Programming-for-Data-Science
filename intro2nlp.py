# -*- coding: utf-8 -*-
"""introNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16HWN92CDOJKGTWVCVt0vCJZ57n-7HhrK
"""

import nltk

def do_downloads():
  nltk.download('punkt')
  nltk.download('averaged_perceptron_tagger')
  nltk.download('maxent_ne_chunker')           
  nltk.download('words')
  nltk.download('wordnet')

def nltk_tokenize_demo(text):
  for sentence in nltk.sent_tokenize(text):
    tokens = nltk.word_tokenize(sentence)
    print(tokens)
    
demo = "This is a simple sentence. Followed by another!"
#nltk_tokenize_demo(demo)

def nltk_pos_demo(text):
  for sent in nltk.sent_tokenize(text):
    tokens = nltk.word_tokenize(sent)
    tagged = nltk.pos_tag(tokens)
    for t in tagged:
      print(t)

demo = "This is a simple sentence. Followed by another!"
#nltk_pos_demo(demo)

def nltk_ne_demo(text):
  for sent in nltk.sent_tokenize(text):
    tokens = nltk.word_tokenize(sent)
    tagged = nltk.pos_tag(tokens)
    for chunk in nltk.ne_chunk(tagged):
      print(chunk)

demo = 'San Francisco considers banning sidewalk delivery robots'
#nltk_ne_demo(demo)

s1 = 'San Francisco considers banning sidewalk delivery robots'
s2 = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
#nltk_ne_demo(s2)

def nltk_find_people(text):
  for sent in nltk.sent_tokenize(text):
    tagged = nltk.pos_tag(nltk.word_tokenize(sent))
    for chunk in nltk.ne_chunk(tagged):
      if hasattr(chunk, 'label') and chunk.label() == 'PERSON':
        name = ' '.join(c[0] for c in chunk)
        print(name)

s3 = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
nltk_find_people(s3)

import spacy

nlp = spacy.load('en_core_web_sm')

def spacy_tokenizer_demo(text):
  tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)
  for token in tokenizer(text):
    print(token)

s = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
#spacy_tokenizer_demo(s)

def spacy_pos_demo(text):
  doc = nlp(text)
  for token in doc:
    print(token.text, token.pos_)
    
s = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
#spacy_pos_demo(s)

def spacy_ner_demo(text):
  doc = nlp(text)
  for entity in doc.ents:
    print(entity.text, entity.label_)
    
s = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
#spacy_ner_demo(s)

def spacy_ner2_demo(text):
  doc = nlp(text)
  for t in doc:
    print(t.text,t.lemma_,t.pos_, t.tag_, t.dep_,
          t.shape_, t.is_alpha, t.is_stop)
#spacy_ner2_demo(s)

def spacy_visualize_demo(text):

  doc = nlp(text)

  spacy.displacy.render(doc, style='dep', jupyter=True)
  spacy.displacy.render(doc, style='ent', jupyter=True)

s = 'In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.'
#spacy_visualize_demo(s)

nltk.download('stopwords') 
from nltk.corpus import stopwords

def stop_word_demo():
  stop_words = stopwords.words('english')
  print("nltk", stop_words)

  #Spacy
  print("spacy", nlp.Defaults.stop_words)

  s1 = set(stop_words)
  s2 = set(nlp.Defaults.stop_words)
  print(s1 ^ s2)

from nltk.util import ngrams

def ngram_demo():
  doc = nlp('In San Francisco, Aunt Polly considers paying sidewalk delivery robots $20.00.')

  grams = ngrams(doc, 3)
  for g in grams:
    print(g)

#ngram_demo()

nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def sentiment_demo():
  sentiment_analyzer = SentimentIntensityAnalyzer()
  def polarity_scores(doc):
    return sentiment_analyzer.polarity_scores(doc.text)
 
  nlp = spacy.load('en_core_web_sm')

  doc1 = nlp("INFO 490 is so fun.")
  doc2 = nlp("INFO 490 is so awful.")
  doc3 = nlp("INFO 490 is so fun that I can't wait to take the follow on course!")
  doc4 = nlp("INFO 490 is so awful that I am glad there's not a follow on course!")

  print(polarity_scores(doc1)) # most positive
  print(polarity_scores(doc2)) # most negative
  print(polarity_scores(doc3)) # mostly positive, neutral
  print(polarity_scores(doc4)) # mostly negative, a little positive too

#sentiment_demo()

def stem_and_lemm_demo():

  words = ["game","gaming","gamed","games","gamer","grows","fairly","nonsensical"]

  ps  = nltk.stem.PorterStemmer()
  sno = nltk.stem.SnowballStemmer('english')
  lan = nltk.stem.lancaster.LancasterStemmer()
 
  for word in words:
    base  = ps.stem(word)
    sbase = sno.stem(word)
    lbase = lan.stem(word)
  
    s = ''
    if (sbase != base):
      s += "(or {})".format(sbase)
    if (lbase != base and lbase != sbase):
      s += "(or {})".format(lbase)
  
    print("{:11s} stems to {:s} {}".format(word, base, s))

def nlkt_wordnet_demo():
  lemma = nltk.stem.WordNetLemmatizer()
  print(lemma.lemmatize('dogs'))

def spacy_lemma_demo():
  text = "I was seeing the screeching ghosts among the trees. Whooping and screeching is all I could do."

  doc = nlp(text.lower())
  for t in doc:
    print(t.text, t.lemma_)